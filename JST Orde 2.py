# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tposxLVWFWrTqPBsa5UVpTZ7oTP798Qr
"""

import tensorflow as tf

class SecondOrderDNN(tf.keras.Model):
    def __init__(self, num_classes):
        super(SecondOrderDNN, self).__init__()
        self.dense1 = tf.keras.layers.Dense(64, activation='relu')
        self.dense2 = tf.keras.layers.Dense(64, activation='relu')
        self.dense3 = tf.keras.layers.Dense(num_classes, activation='softmax')

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        return self.dense3(x)

# Load your dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Normalize pixel values
x_train = x_train / 255.0
x_test = x_test / 255.0

# Flatten the images
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# Convert labels to one-hot encoding
y_train = tf.one_hot(y_train, depth=10)
y_test = tf.one_hot(y_test, depth=10)

# Create an instance of the model
model = SecondOrderDNN(num_classes=10)

# Define loss function and optimizer
loss_fn = tf.keras.losses.CategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

# Metrics to track during training (optional)
train_loss = tf.keras.metrics.Mean()
train_accuracy = tf.keras.metrics.CategoricalAccuracy()

# Training loop
epochs = 2
batch_size = 32
steps_per_epoch = x_train.shape[0] // batch_size

for epoch in range(epochs):
    print(f"Epoch {epoch+1}/{epochs}")
    for step in range(steps_per_epoch):
        # Get batch data
        x_batch = x_train[step * batch_size: (step + 1) * batch_size]
        y_batch = y_train[step * batch_size: (step + 1) * batch_size]

        # Forward pass
        with tf.GradientTape() as tape:
            logits = model(x_batch, training=True)
            loss_value = loss_fn(y_batch, logits)

        # Backward pass
        grads = tape.gradient(loss_value, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        # Update metrics
        train_loss(loss_value)
        train_accuracy(y_batch, logits)

        # Print training progress every few steps
        if step % 100 == 0:
            print(f"Step {step}/{steps_per_epoch}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result()}")

    # Evaluate the model on the test set
    test_logits = model(x_test, training=False)
    test_loss = loss_fn(y_test, test_logits)
    test_accuracy = tf.keras.metrics.CategoricalAccuracy()(y_test, test_logits)
    print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

    # Reset metrics for the next epoch
    train_loss.reset_states()
    train_accuracy.reset_states()